{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a2c6c7-ffb9-4e03-a099-6dda6a4f1966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 12:29:25.852318: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-29 12:29:26.225125: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-29 12:29:26.297724: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-08-29 12:29:27.424728: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-29 12:29:27.424820: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-29 12:29:27.424826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-08-29 12:29:32.424395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-29 12:29:32.428351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-29 12:29:32.428383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-29 12:29:32.429327: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-29 12:29:32.431448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-29 12:29:32.431507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-29 12:29:32.431524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-29 12:29:35.757017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-29 12:29:35.757421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-29 12:29:35.757431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-29 12:29:35.757472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-29 12:29:35.757523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1241 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-08-29 12:29:38.224424: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
    "model = TFT5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cfb4e1e-f047-4ded-aa07-26088d43943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text =\"\"\" \n",
    "President Donald Trump's physician, Navy Cmdr Dr. Sean Conley, held a second medical briefing that again raised more questions \n",
    "than answers about the President's condition.In another jarring news conference on Sunday, Trump's doctors said that even \n",
    "though the President has had at least two concerning drops in oxygen levels, they are hoping he could be discharged as early as\n",
    "tomorrow from Walter Reed National Military Medical Center.Conley and other doctors involved in the President's care offered \n",
    "some more information about the President's condition -- but there were still significant gaps that made it hard to decipher\n",
    "the full picture.Conley failed to answer basic questions about the President's condition and admitted that he had omitted \n",
    "those alarming drops in the President's oxygen levels during a news conference Saturday because he wanted to \"reflect the upbeat \n",
    "attitude\" that the team and the President had about his condition and didn't want \"to give any information that might steer the \n",
    "course of illness in another direction.\"Conley acknowledged that his evasive answers \"came off that we were trying to hide \n",
    "something\" but said that \"wasn't necessarily true,\" adding that the President is \"doing really well\" and is responding to treatment.\n",
    "During the briefing Sunday, Conley acknowledged that the President has experienced \"two episodes of transient drops in his oxygen \n",
    "saturation\" and said the team debated the reasons for that and whether to intervene. He said the President was given supplemental oxygen and has also been treated with the steroid dexamethasone, and his current blood oxygen level is 98%.\n",
    "But Conley refused to say how low the President's blood oxygen levels had dropped. When asked if they had dropped below 90, \n",
    "he replied, \"We don't have any recordings here of that.\" Pressed again on whether they had dropped below 90, Conley said \n",
    "the President's blood oxygen levels didn't get down into \"the low 80s.\"\n",
    "He offered no detail about what X-rays or CT scans have shown about whether there has been any damage to the President's lungs.\n",
    "\"There's some expected findings, but nothing of any major clinical concern,\" Conley said, not explaining whether they were \n",
    "expected findings in a normal patient or a Covid-19 patient.\n",
    "Some seven months into a pandemic that has killed more than 209,000 Americans, the nation is now facing a \n",
    "grave governing crisis with its commander in chief hospitalized -- his condition hinging on his progress over \n",
    "the coming days -- as the White House events of the past week serve as a textbook example of how not to handle a deadly virus\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ae4bdb-c7aa-4599-a2a5-f5350f27bfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "Text = text.strip().replace(\"\\n\",\"\")\n",
    "Preprocessed_text = \"summarize: \"+Text\n",
    "tokens_input = tokenizer.encode(Preprocessed_text,return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0931466-c714-459a-ac10-367a22a2ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 12:04:28.870464: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x55a9f9103ee0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-29 12:04:28.870531: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Ti Laptop GPU, Compute Capability 8.6\n",
      "2023-08-29 12:04:28.986742: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-29 12:04:29.136132: W tensorflow/compiler/xla/service/gpu/nvptx_helper.cc:56] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-11.2\n",
      "  /usr/local/cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2023-08-29 12:04:29.272939: I tensorflow/compiler/jit/xla_compilation_cache.cc:476] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "summary_ids = model.generate(tokens_input,\n",
    "                             min_length=60,\n",
    "                             max_length=80,\n",
    "                             )\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea1591e-7200-4b4d-8346-8f1f5d087d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<pad> president's physician, navy cmdr, held a second medical briefing on the president's condition. he said he omitted the alarming drops in the president's oxygen levels. he said the president is responding to treatment.</s>\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1ce0b5-8401-405b-a8da-6b45d790eb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summarization': {'early_stopping': True, 'length_penalty': 2.0, 'max_length': 200, 'min_length': 30, 'no_repeat_ngram_size': 3, 'num_beams': 4, 'prefix': 'summarize: '}, 'translation_en_to_de': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to German: '}, 'translation_en_to_fr': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to French: '}, 'translation_en_to_ro': {'early_stopping': True, 'max_length': 300, 'num_beams': 4, 'prefix': 'translate English to Romanian: '}}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.task_specific_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293836d-a299-4a49-acec-99cc3b02bed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
