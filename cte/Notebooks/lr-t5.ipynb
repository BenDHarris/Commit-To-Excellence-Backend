{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lscr/.pyenv/versions/3.10.6/envs/cte/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remove Romanian translation (abandoned)\\n\\nThe...</td>\n",
       "      <td>diff --git a/translations.md b/translations.md...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Add link to Real Python on editor</td>\n",
       "      <td>diff --git a/first_steps.md b/first_steps.md\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fix typo</td>\n",
       "      <td>diff --git a/README.md b/README.md\\nindex 85a9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Remove whitespace to fix header rendering\\n\\nT...</td>\n",
       "      <td>diff --git a/problem_solving.md b/problem_solv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fix sentence grammar\\n\\nThanks to John Thomas.</td>\n",
       "      <td>diff --git a/basics.md b/basics.md\\nindex 3729...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message  \\\n",
       "0  Remove Romanian translation (abandoned)\\n\\nThe...   \n",
       "1                  Add link to Real Python on editor   \n",
       "2                                           Fix typo   \n",
       "3  Remove whitespace to fix header rendering\\n\\nT...   \n",
       "4     Fix sentence grammar\\n\\nThanks to John Thomas.   \n",
       "\n",
       "                                                Diff  \n",
       "0  diff --git a/translations.md b/translations.md...  \n",
       "1  diff --git a/first_steps.md b/first_steps.md\\n...  \n",
       "2  diff --git a/README.md b/README.md\\nindex 85a9...  \n",
       "3  diff --git a/problem_solving.md b/problem_solv...  \n",
       "4  diff --git a/basics.md b/basics.md\\nindex 3729...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data and minor preprocessing\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "df = df.drop(columns=[\"Unnamed: 0\", \"Repository\"]).dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remove Romanian translation (abandoned)\\n\\nThe...</td>\n",
       "      <td>diff --git a/translations.md b/translations.md...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Add link to Real Python on editor</td>\n",
       "      <td>diff --git a/first_steps.md b/first_steps.md\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fix typo</td>\n",
       "      <td>diff --git a/README.md b/README.md\\nindex 85a9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Remove whitespace to fix header rendering\\n\\nT...</td>\n",
       "      <td>diff --git a/problem_solving.md b/problem_solv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fix sentence grammar\\n\\nThanks to John Thomas.</td>\n",
       "      <td>diff --git a/basics.md b/basics.md\\nindex 3729...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>Create a list of written assignments\\n\\nWritte...</td>\n",
       "      <td>diff --git a/written assignments/a list of wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Create text_based_calculator\\n\\nThe first codi...</td>\n",
       "      <td>diff --git a/coding projects/text_based_calcul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>Create 1_numbers_in_python\\n\\nPower point file...</td>\n",
       "      <td>diff --git a/power_points/1_numbers_in_python ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Add files via upload\\n\\nFirst lesson of the co...</td>\n",
       "      <td>diff --git a/1_numbers_in_python.py b/1_number...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>Add files via upload\\n\\nCourse syllabus versio...</td>\n",
       "      <td>diff --git a/python3_course_syllabus.docx b/py...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>461 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Message  \\\n",
       "0    Remove Romanian translation (abandoned)\\n\\nThe...   \n",
       "1                    Add link to Real Python on editor   \n",
       "2                                             Fix typo   \n",
       "3    Remove whitespace to fix header rendering\\n\\nT...   \n",
       "4       Fix sentence grammar\\n\\nThanks to John Thomas.   \n",
       "..                                                 ...   \n",
       "535  Create a list of written assignments\\n\\nWritte...   \n",
       "536  Create text_based_calculator\\n\\nThe first codi...   \n",
       "537  Create 1_numbers_in_python\\n\\nPower point file...   \n",
       "538  Add files via upload\\n\\nFirst lesson of the co...   \n",
       "539  Add files via upload\\n\\nCourse syllabus versio...   \n",
       "\n",
       "                                                  Diff  \n",
       "0    diff --git a/translations.md b/translations.md...  \n",
       "1    diff --git a/first_steps.md b/first_steps.md\\n...  \n",
       "2    diff --git a/README.md b/README.md\\nindex 85a9...  \n",
       "3    diff --git a/problem_solving.md b/problem_solv...  \n",
       "4    diff --git a/basics.md b/basics.md\\nindex 3729...  \n",
       "..                                                 ...  \n",
       "535  diff --git a/written assignments/a list of wri...  \n",
       "536  diff --git a/coding projects/text_based_calcul...  \n",
       "537  diff --git a/power_points/1_numbers_in_python ...  \n",
       "538  diff --git a/1_numbers_in_python.py b/1_number...  \n",
       "539  diff --git a/python3_course_syllabus.docx b/py...  \n",
       "\n",
       "[461 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"Diff\"].str.len() < 5000]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Message', 'Diff'],\n",
       "    num_rows: 461\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataset objects\n",
    "data = Dataset.from_pandas(df, preserve_index=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Message', 'Diff'],\n",
       "        num_rows: 356\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Message', 'Diff'],\n",
       "        num_rows: 55\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Message', 'Diff'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_test = data.train_test_split(test_size=50)\n",
    "data_train_val = data_train_test[\"train\"].train_test_split(test_size=55)\n",
    "\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": data_train_val[\"train\"],\n",
    "    \"validation\": data_train_val[\"test\"],\n",
    "    \"test\": data_train_test[\"test\"]\n",
    "})\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.32.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Hugging Face \n",
    "import transformers\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess + Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate Tokenizer \n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Preprocess + Tokenizer Params\n",
    "prefix = \"summarize: \"\n",
    "max_feature_length = 256\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # Process + tokenize features\n",
    "    inputs = [prefix + doc for doc in examples[\"Diff\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_feature_length, truncation=True)\n",
    "    \n",
    "    print(type(model_inputs))\n",
    "    \n",
    "    # tokenize targets\n",
    "    labels = tokenizer(examples[\"Message\"], max_length=max_target_length, truncation=True)\n",
    "        \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 356/356 [00:00<00:00, 1457.50 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 55/55 [00:00<00:00, 2891.90 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 2210.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Message', 'Diff', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 356\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Message', 'Diff', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 55\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Message', 'Diff', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = ds.map(preprocess_data, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import evaluate # add to requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params and args\n",
    "\n",
    "batch_size = 8\n",
    "model_name = \"t5-small-cte\"\n",
    "model_dir = f\"../../saved_models/{model_name}\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    # fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load metric? with evaluation function\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Takes a tuple of predictions and reference labels as input, \n",
    "    and outputs a dictionary of metrics computed over the inputs.\"\"\"\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 1.08MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model from checkpoint - loads up T5 with weights and architecture\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thing that does the training - honestly don't know why it's this hard.\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/135 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 74%|███████▍  | 100/135 [04:23<01:03,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2717, 'learning_rate': 1.037037037037037e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 74%|███████▍  | 100/135 [04:33<01:03,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.487128973007202, 'eval_rouge1': 0.216, 'eval_rouge2': 0.1481, 'eval_rougeL': 0.1954, 'eval_rougeLsum': 0.1952, 'eval_gen_len': 17.9455, 'eval_runtime': 9.8555, 'eval_samples_per_second': 5.581, 'eval_steps_per_second': 0.71, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [05:50<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 350.6779, 'train_samples_per_second': 3.046, 'train_steps_per_second': 0.385, 'train_loss': 4.180418113425926, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=135, training_loss=4.180418113425926, metrics={'train_runtime': 350.6779, 'train_samples_per_second': 3.046, 'train_steps_per_second': 0.385, 'train_loss': 4.180418113425926, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train, apparently\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../../saved_models/t5-small-cte-lorcan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way 1: Use pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"../../saved_models/t5-small-cte-lorcan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"''Check if word exists in English dictionary'' response = requests.get(f'https://wagon-dictionary\"}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\n",
    "    \"@staticmethod\\n def __check_dictionary(word):\\n '''Check if word exists in English dictionary'''\\n response = requests.get(f'https://wagon-dictionary.herokuapp.com/{word}')\\n json_response = response.json()\\n        return json_response[\\\"found\\\"]\", max_length=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way 2: The hard way I think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"''Check if word exists in English dictionary''' response = requests.get(f'https://wagon-dictionary.herokuapp.com/word''\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"@staticmethod\\n def __check_dictionary(word):\\n '''Check if word exists in English dictionary'''\\n response = requests.get(f'https://wagon-dictionary.herokuapp.com/{word}')\\n json_response = response.json()\\n        return json_response[\\\"found\\\"]\"\n",
    "\n",
    "inputs = [\"summarize: \" + pred_text]\n",
    "\n",
    "inputs = tokenizer(inputs, max_length=max_feature_length, truncation=True, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=64)\n",
    "decoder_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more data on a bigger model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base inference on T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[114, 25, 281, 250, 228, 142, 104, 89, 432, 32]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_list = pd.read_csv(\"data2.csv\").dropna()\n",
    "as_list = as_list[\"Diff\"].tolist()\n",
    "\n",
    "import random\n",
    "\n",
    "i = 0\n",
    "num_indexes = 10\n",
    "rand_indexes = []\n",
    "\n",
    "\n",
    "while i < num_indexes:\n",
    "    r = random.randint(0, len(as_list) - 1)\n",
    "    \n",
    "    if r not in rand_indexes:\n",
    "        rand_indexes.append(r)\n",
    "        i += 1\n",
    "\n",
    "rand_indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 assert statement is used to assert that something is true . if the assert statement fails, an As\n",
      "1 - 11 is represented in bits by 1011 which when right shifted by 1 bit gives 101\n",
      "2 a/fabfile.py index 8cdcae9a..1f0432f8 100755 .\n",
      "3 a/07-operators-expressions.md index c99c20d5..a0b5d2e\n",
      "4 git a/frontpage.asciidoc +++ b/front page.acidoc @@ \n",
      "5 git a/programs/backup_ver3.py index f6ff4fc3..d6\n",
      "6 ## Chinese +**The following URLs are unavailable now . translations are available at http://woodpecker.org.\n",
      "7 a/data_structures.md b243b025..c4c4ab40 100644 .\n",
      "8 anyhoo, below is a comprehensive summary of all of the cool coding projects that accompanies the\n",
      "9 a/.github/workflows/main.yml index 13833b57..a72a5811\n"
     ]
    }
   ],
   "source": [
    "t5_small = pipeline(\"summarization\", \"t5-small\")\n",
    "\n",
    "for i, ind in enumerate(rand_indexes):\n",
    "    print(i, t5_small(as_list[ind], max_length=30)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning of data is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 python >>> mylist = ['item']  pytheon = \n",
      "1 11 is represented in bits by 1011 which when right shifted by 1 bit gives 101which is\n",
      "2 # TODO Use a proper category instead + \"search\": \"python_en\", \"offset\": offset\n",
      "3 a/07-operators-expressions.md index c99c20d5..a0b5d2e\n",
      "4 b/frontpage.asciidoc @@ -60,6 +60,11 @@ A Byte of\n",
      "5 if len(comment) == 0: target = today + os.sep + now + '\n",
      "6 b/translations.md index d75b6a4c..dc102e1a 100644\n",
      "7 # parentheses not required but are a good idea print('Number of cages in the new zoo is\n",
      "8 # Text Based Calculator -The massive headline pretty much said it all. Anyhoo, below is a comprehensive summary of all\n",
      "9 b/.github/workflows/main.yml index 13833b57..a72a5811\n"
     ]
    }
   ],
   "source": [
    "flan_t5_small = pipeline(\"summarization\", \"google/flan-t5-small\")\n",
    "\n",
    "for i, ind in enumerate(rand_indexes):\n",
    "    print(i, flan_t5_small(as_list[ind], max_length=30)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
