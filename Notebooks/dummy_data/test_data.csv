,message,diff
1,"Merge pull request #18243 from mdhaber/gh18231

TST: stats.dunnett: fix seed in test_shapes","diff --git a/scipy/stats/tests/test_multicomp.py b/scipy/stats/tests/test_multicomp.py
index 6144db23dc58..c85d95ebbb16 100644
--- a/scipy/stats/tests/test_multicomp.py
+++ b/scipy/stats/tests/test_multicomp.py
@@ -390,12 +390,13 @@ def test_raises(self):
         with pytest.raises(ValueError, match=""Confidence level must""):
             res.confidence_interval(confidence_level=3)
 
+    @pytest.mark.filterwarnings(""ignore:Computation of the confidence"")
     @pytest.mark.parametrize('n_samples', [1, 2, 3])
     def test_shapes(self, n_samples):
         rng = np.random.default_rng(689448934110805334)
         samples = rng.normal(size=(n_samples, 10))
         control = rng.normal(size=10)
-        res = stats.dunnett(*samples, control=control)
+        res = stats.dunnett(*samples, control=control, random_state=rng)
         assert res.statistic.shape == (n_samples,)
         assert res.pvalue.shape == (n_samples,)
         ci = res.confidence_interval()
"
3,TST: stats.dunnett: fix seed and filter warnings in test_shapes,"diff --git a/scipy/stats/tests/test_multicomp.py b/scipy/stats/tests/test_multicomp.py
index 6144db23dc58..c85d95ebbb16 100644
--- a/scipy/stats/tests/test_multicomp.py
+++ b/scipy/stats/tests/test_multicomp.py
@@ -390,12 +390,13 @@ def test_raises(self):
         with pytest.raises(ValueError, match=""Confidence level must""):
             res.confidence_interval(confidence_level=3)
 
+    @pytest.mark.filterwarnings(""ignore:Computation of the confidence"")
     @pytest.mark.parametrize('n_samples', [1, 2, 3])
     def test_shapes(self, n_samples):
         rng = np.random.default_rng(689448934110805334)
         samples = rng.normal(size=(n_samples, 10))
         control = rng.normal(size=10)
-        res = stats.dunnett(*samples, control=control)
+        res = stats.dunnett(*samples, control=control, random_state=rng)
         assert res.statistic.shape == (n_samples,)
         assert res.pvalue.shape == (n_samples,)
         ci = res.confidence_interval()
"
4,"Merge pull request #18240 from uniontech-lilinjie/main

fix typo","diff --git a/scipy/stats/_rvs_sampling.py b/scipy/stats/_rvs_sampling.py
index 8598fb099e10..a8152358ec6f 100644
--- a/scipy/stats/_rvs_sampling.py
+++ b/scipy/stats/_rvs_sampling.py
@@ -63,7 +63,7 @@ def rvs_ratio_uniforms(pdf, umax, vmin, vmax, size=1, c=0, random_state=None):
     The algorithm is not changed if one replaces `pdf` by k * `pdf` for any
     constant k > 0. Thus, it is often convenient to work with a function
     that is proportional to the probability density function by dropping
-    unneccessary normalization factors.
+    unnecessary normalization factors.
 
     Intuitively, the method works well if `A` fills up most of the
     enclosing rectangle such that the probability is high that `(U, V)`
"
5,"DOC: add info to use codespaces.

[skip actions] [skip cirrus] [skip azp]","diff --git a/doc/source/dev/dev_quickstart.rst b/doc/source/dev/dev_quickstart.rst
index 12fdf093a8a2..f1071d462077 100644
--- a/doc/source/dev/dev_quickstart.rst
+++ b/doc/source/dev/dev_quickstart.rst
@@ -108,3 +108,23 @@ Other workflows
 
 This is only one possible way to set up your development environment out of
 many. For more detailed instructions, see the :ref:`contributor-toc`.
+
+.. note::
+
+    If you are having trouble building SciPy from source or setting up your
+    local development environment, you can try to build SciPy with GitHub
+    Codespaces. It allows you to create the correct development environment
+    right in your browser, reducing the need to install local development
+    environments and deal with incompatible dependencies.
+
+    If you have good internet connectivity and want a temporary set-up, it is
+    often faster to work on SciPy in a Codespaces environment. For
+    documentation on how to get started with Codespaces, see
+    `the Codespaces docs <https://docs.github.com/en/codespaces>`__.
+    When creating a codespace for the ``scipy/scipy`` repository, the default
+    2-core machine type works; 4-core will build and work a bit faster (but of
+    course at a cost of halving your number of free usage hours). Once your
+    codespace has started, you can run ``mamba activate scipy-dev`` and your
+    development environment is completely set up - you can then follow the
+    relevant parts of the SciPy documentation to build, test, develop, write
+    docs, and contribute to SciPy.
"
6,MAINT: add devcontainer configuration,"diff --git a/.devcontainer/devcontainer.json b/.devcontainer/devcontainer.json
new file mode 100644
index 000000000000..a31be7931370
--- /dev/null
+++ b/.devcontainer/devcontainer.json
@@ -0,0 +1,17 @@
+{
+	// More info about Features: https://containers.dev/features
+	""image"": ""mcr.microsoft.com/devcontainers/universal:2"",
+	""features"": {},
+
+	""onCreateCommand"": "".devcontainer/setup.sh"",
+	""postCreateCommand"": """",
+
+	""customizations"": {
+		""vscode"": {
+			""extensions"": [
+				""ms-python.python""
+			],
+			""settings"": {}
+		}
+	}
+}
diff --git a/.devcontainer/setup.sh b/.devcontainer/setup.sh
new file mode 100644
index 000000000000..a07a91a52b37
--- /dev/null
+++ b/.devcontainer/setup.sh
@@ -0,0 +1,13 @@
+#!/bin/bash
+
+set -e
+
+curl micro.mamba.pm/install.sh | bash
+
+conda init --all
+micromamba shell init -s bash
+micromamba env create -f environment.yml --yes
+# Note that `micromamba activate scipy-dev` doesn't work, it must be run by the
+# user (same applies to `conda activate`)
+
+git submodule update --init
"
8,MAINT: remove unused CI folder,"diff --git a/tools/ci/README.txt b/tools/ci/README.txt
deleted file mode 100644
index 65ea3e877637..000000000000
--- a/tools/ci/README.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-Continuous integration support files
-------------------------------------
-
"
9,MAINT: update typing of Rotation (#18237),"diff --git a/scipy/spatial/transform/_rotation.pyi b/scipy/spatial/transform/_rotation.pyi
index 28aa33aa21d5..09d625429a63 100644
--- a/scipy/spatial/transform/_rotation.pyi
+++ b/scipy/spatial/transform/_rotation.pyi
@@ -18,14 +18,14 @@ class Rotation:
     @classmethod
     def from_matrix(cls, matrix: npt.ArrayLike) -> Rotation: ...
     @classmethod
-    def from_rotvec(cls, rotvec: npt.ArrayLike) -> Rotation: ...
+    def from_rotvec(cls, rotvec: npt.ArrayLike, degrees: bool = ...) -> Rotation: ...
     @classmethod
     def from_euler(cls, seq: str, angles: float | npt.ArrayLike, degrees: bool = ...) -> Rotation: ...
     @classmethod
     def from_mrp(cls, mrp: npt.ArrayLike) -> Rotation: ...
     def as_quat(self) -> np.ndarray: ...
     def as_matrix(self) -> np.ndarray: ...
-    def as_rotvec(self) -> np.ndarray: ...
+    def as_rotvec(self, degrees: bool = ...) -> np.ndarray: ...
     def as_euler(self, seq: str, degrees: bool = ...) -> np.ndarray: ...
     def as_mrp(self) -> np.ndarray: ...
     @classmethod
"
11,DOC: add guidance on how to make a dataclass for result objects (#18221),"diff --git a/doc/source/dev/missing-bits.rst b/doc/source/dev/missing-bits.rst
index c2f65a05c3f5..43f877c57af0 100644
--- a/doc/source/dev/missing-bits.rst
+++ b/doc/source/dev/missing-bits.rst
@@ -87,6 +87,19 @@ private return classes, please see  how `~scipy.stats.binomtest` summarizes
 `~scipy.stats._result_classes.BinomTestResult` and links to its documentation,
 and note that ``BinomTestResult`` cannot be imported from `~scipy.stats`.
 
+Depending on the complexity of ""MyResultObject"", a normal class or a dataclass
+can be used. When using dataclasses, do not use ``dataclasses.make_dataclass``,
+instead use a proper declaration. This allows autocompletion to list all
+the attributes of the result object and improves static analysis.
+Finally, hide private attributes if any::
+
+    @dataclass
+    class MyResultObject:
+        statistic: np.ndarray
+        pvalue: np.ndarray
+        confidence_interval: ConfidenceInterval
+        _rho: np.ndarray = field(repr=False)
+
 
 Test functions from `numpy.testing`
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
"
12,"fix typo

Signed-off-by: lilinjie <lilinjie@uniontech.com>","diff --git a/scipy/stats/_rvs_sampling.py b/scipy/stats/_rvs_sampling.py
index 8598fb099e10..a8152358ec6f 100644
--- a/scipy/stats/_rvs_sampling.py
+++ b/scipy/stats/_rvs_sampling.py
@@ -63,7 +63,7 @@ def rvs_ratio_uniforms(pdf, umax, vmin, vmax, size=1, c=0, random_state=None):
     The algorithm is not changed if one replaces `pdf` by k * `pdf` for any
     constant k > 0. Thus, it is often convenient to work with a function
     that is proportional to the probability density function by dropping
-    unneccessary normalization factors.
+    unnecessary normalization factors.
 
     Intuitively, the method works well if `A` fills up most of the
     enclosing rectangle such that the probability is high that `(U, V)`
"
14,"DOC: special: In the tklmbda docstring, add comparison to scipy.stats.tukeylambda.cdf","diff --git a/scipy/special/_add_newdocs.py b/scipy/special/_add_newdocs.py
index c0dbfac302d3..88415d31f7b9 100644
--- a/scipy/special/_add_newdocs.py
+++ b/scipy/special/_add_newdocs.py
@@ -13046,6 +13046,28 @@ def add_newdoc(name, doc):
     >>> plt.tight_layout()
     >>> plt.show()
 
+    The CDF of the Tukey lambda distribution is also implemented as the
+    ``cdf`` method of `scipy.stats.tukeylambda`.  In the following,
+    ``tukeylambda.cdf(x, -0.5)`` and ``tklmbda(x, -0.5)`` compute the
+    same values:
+
+    >>> from scipy.stats import tukeylambda
+    >>> x = np.linspace(-2, 2, 9)
+
+    >>> tukeylambda.cdf(x, -0.5)
+    array([0.21995157, 0.27093858, 0.33541677, 0.41328161, 0.5       ,
+           0.58671839, 0.66458323, 0.72906142, 0.78004843])
+
+    >>> tklmbda(x, -0.5)
+    array([0.21995157, 0.27093858, 0.33541677, 0.41328161, 0.5       ,
+           0.58671839, 0.66458323, 0.72906142, 0.78004843])
+
+    The implementation in ``tukeylambda`` also provides location and scale
+    parameters, and other methods such as ``pdf()`` (the probability
+    density function) and ``ppf()`` (the inverse of the CDF), so for
+    working with the Tukey lambda distribution, ``tukeylambda`` is more
+    generally useful.  The primary advantage of ``tklmbda`` is that it is
+    significantly faster than ``tukeylambda.cdf``.
     """""")
 
 add_newdoc(""wofz"",
"
15,"ENH: stats.geometric.entropy: implement analytical formula (#18228)

* ENH: stats.geometric.entropy: implement analytical formula","diff --git a/scipy/stats/_discrete_distns.py b/scipy/stats/_discrete_distns.py
index 9cfa6d76a04a..26e90e0b6ac3 100644
--- a/scipy/stats/_discrete_distns.py
+++ b/scipy/stats/_discrete_distns.py
@@ -441,6 +441,9 @@ def _stats(self, p):
         g2 = np.polyval([1, -6, 6], p)/(1.0-p)
         return mu, var, g1, g2
 
+    def _entropy(self, p):
+        return -np.log(p) - np.log1p(-p) * (1.0-p) / p
+
 
 geom = geom_gen(a=1, name='geom', longname=""A geometric"")
 
diff --git a/scipy/stats/tests/test_distributions.py b/scipy/stats/tests/test_distributions.py
index 76c0d6892eda..d4379f033861 100644
--- a/scipy/stats/tests/test_distributions.py
+++ b/scipy/stats/tests/test_distributions.py
@@ -999,6 +999,12 @@ def test_ppf_underflow(self):
         # this should not underflow
         assert_allclose(stats.geom.ppf(1e-20, 1e-20), 1.0, atol=1e-14)
 
+    def test_entropy_gh18226(self):
+        # gh-18226 reported that `geom.entropy` produced a warning and
+        # inaccurate output for small p. Check that this is resolved.
+        h = stats.geom(0.0146).entropy()
+        assert_allclose(h, 5.219397961962308, rtol=1e-15)
+
 
 class TestPlanck:
     def setup_method(self):
"
16,MAINT: stats.ecdf: kwargs -> matplotlib_kwargs,"diff --git a/scipy/stats/_survival.py b/scipy/stats/_survival.py
index 5fc55a1542f6..4454a5ea29d6 100644
--- a/scipy/stats/_survival.py
+++ b/scipy/stats/_survival.py
@@ -71,7 +71,7 @@ def evaluate(self, x):
         """"""
         return self._f(x)
 
-    def plot(self, ax=None, **kwargs):
+    def plot(self, ax=None, **matplotlib_kwargs):
         """"""Plot the empirical distribution function
 
         Available only if ``matplotlib`` is installed.
@@ -81,7 +81,7 @@ def plot(self, ax=None, **kwargs):
         ax : matplotlib.axes.Axes
             Axes object to draw the plot onto, otherwise uses the current Axes.
 
-        **kwargs : dict, optional
+        **matplotlib_kwargs : dict, optional
             Keyword arguments passed directly to `matplotlib.axes.Axes.step`.
             Unless overridden, ``where='post'``.
 
@@ -100,14 +100,14 @@ def plot(self, ax=None, **kwargs):
             import matplotlib.pyplot as plt
             ax = plt.gca()
 
-        kwds = {'where': 'post'}
-        kwds.update(kwargs)
+        kwargs = {'where': 'post'}
+        kwargs.update(matplotlib_kwargs)
 
         delta = np.ptp(self.quantiles)*0.05  # how far past sample edge to plot
         q = self.quantiles
         q = [q[0] - delta] + list(q) + [q[-1] + delta]
 
-        return ax.step(q, self.evaluate(q), **kwds)
+        return ax.step(q, self.evaluate(q), **kwargs)
 
     def confidence_interval(self, confidence_level=0.95, *, method='linear'):
         """"""Compute a confidence interval around the CDF/SF point estimate
"
17,MAINT: stats.ecdf: implement suggestions; fix `logrank` plotting,"diff --git a/scipy/stats/_survival.py b/scipy/stats/_survival.py
index f54195904b34..5fc55a1542f6 100644
--- a/scipy/stats/_survival.py
+++ b/scipy/stats/_survival.py
@@ -103,10 +103,9 @@ def plot(self, ax=None, **kwargs):
         kwds = {'where': 'post'}
         kwds.update(kwargs)
 
-        factor = 0.05  # how far past the edges of the sample to plot
-        ptp = np.ptp(self.quantiles)
+        delta = np.ptp(self.quantiles)*0.05  # how far past sample edge to plot
         q = self.quantiles
-        q = [q[0] - ptp*factor] + list(q) + [q[-1] + ptp*factor]
+        q = [q[0] - delta] + list(q) + [q[-1] + delta]
 
         return ax.step(q, self.evaluate(q), **kwds)
 
@@ -283,7 +282,7 @@ def ecdf(sample: npt.ArrayLike | CensoredData) -> ECDFResult:
         The `cdf` and `sf` attributes themselves have the following attributes.
 
         quantiles : ndarray
-            The unique values in the sample.
+            The unique values in the sample that defines the empirical CDF/SF.
         probabilities : ndarray
             The point estimates of the probabilities corresponding with
             `quantiles`.
@@ -614,13 +613,9 @@ def logrank(
     >>> import matplotlib.pyplot as plt
     >>> ax = plt.subplot()
     >>> ecdf_x = stats.ecdf(x)
-    >>> ax.step(np.insert(ecdf_x.x, 0, 0),
-    ...         np.insert(ecdf_x.sf.points, 0, 1),
-    ...         where='post', label='Astrocytoma')
+    >>> ecdf_x.sf.plot(ax, label='Astrocytoma')
     >>> ecdf_y = stats.ecdf(y)
-    >>> ax.step(np.insert(ecdf_y.x, 0, 0),
-    ...         np.insert(ecdf_y.sf.points, 0, 1),
-    ...         ls='-.', where='post', label='Glioblastoma')
+    >>> ecdf_x.sf.plot(ax, label='Glioblastoma')
     >>> ax.set_xlabel('Time to death (weeks)')
     >>> ax.set_ylabel('Empirical SF')
     >>> plt.legend()
"
20,"Merge pull request #18220 from rgommers/fix-ci-conda-issue

CI: clear cache for macOS conda job, and use mamba to install deps","diff --git a/.github/workflows/macos_meson.yml b/.github/workflows/macos_meson.yml
index ba2f3159b920..fc6150287bf9 100644
--- a/.github/workflows/macos_meson.yml
+++ b/.github/workflows/macos_meson.yml
@@ -88,7 +88,7 @@ jobs:
       uses: actions/cache@v3
       env:
         # Increase this value to reset cache if environment.yml has not changed
-        CACHE_NUMBER: 0
+        CACHE_NUMBER: 1
       with:
         path: ${{ env.CONDA }}/envs/scipy-dev
         key:
@@ -105,7 +105,7 @@ jobs:
         conda activate scipy-dev
 
         # optional test dependencies
-        conda install scikit-umfpack scikit-sparse
+        mamba install scikit-umfpack scikit-sparse
 
         # Python.org installers still use 10.9, so let's use that too. Note
         # that scikit-learn already changed to 10.13 in Jan 2021, so increasing
"
21,"CI: clear cache for macOS conda job, and use mamba to install deps

Closes gh-18218

[skip azp] [skip cirrus] [skip circle]","diff --git a/.github/workflows/macos_meson.yml b/.github/workflows/macos_meson.yml
index ba2f3159b920..fc6150287bf9 100644
--- a/.github/workflows/macos_meson.yml
+++ b/.github/workflows/macos_meson.yml
@@ -88,7 +88,7 @@ jobs:
       uses: actions/cache@v3
       env:
         # Increase this value to reset cache if environment.yml has not changed
-        CACHE_NUMBER: 0
+        CACHE_NUMBER: 1
       with:
         path: ${{ env.CONDA }}/envs/scipy-dev
         key:
@@ -105,7 +105,7 @@ jobs:
         conda activate scipy-dev
 
         # optional test dependencies
-        conda install scikit-umfpack scikit-sparse
+        mamba install scikit-umfpack scikit-sparse
 
         # Python.org installers still use 10.9, so let's use that too. Note
         # that scikit-learn already changed to 10.13 in Jan 2021, so increasing
"
22,BUG: spatial.distance.hamming: throw ValueError when w has the wrong size (#18199),"diff --git a/scipy/spatial/distance.py b/scipy/spatial/distance.py
index d7d4e558dc61..b593f0a65bd8 100644
--- a/scipy/spatial/distance.py
+++ b/scipy/spatial/distance.py
@@ -737,6 +737,8 @@ def hamming(u, v, w=None):
     u_ne_v = u != v
     if w is not None:
         w = _validate_weights(w)
+        if w.shape != u.shape:
+            raise ValueError(""'w' should have the same length as 'u' and 'v'."")
     return np.average(u_ne_v, weights=w)
 
 
diff --git a/scipy/spatial/tests/test_distance.py b/scipy/spatial/tests/test_distance.py
index 762f1060cadb..09e0a0b35967 100644
--- a/scipy/spatial/tests/test_distance.py
+++ b/scipy/spatial/tests/test_distance.py
@@ -1935,6 +1935,15 @@ def test_hamming_unequal_length():
     assert_raises(ValueError, whamming, x, y)
 
 
+def test_hamming_unequal_length_with_w():
+    u = [0, 0, 1]
+    v = [0, 0, 1]
+    w = [1, 0, 1, 0]
+    msg = ""'w' should have the same length as 'u' and 'v'.""
+    with assert_raises(ValueError, match=msg):
+        whamming(u, v, w)
+
+
 def test_hamming_string_array():
     # https://github.com/scikit-learn/scikit-learn/issues/4014
     a = np.array(['eggs', 'spam', 'spam', 'eggs', 'spam', 'spam', 'spam',
"
23,MAINT: cluster: pass check_finite to the vq() call of kmeans2(),"diff --git a/scipy/cluster/vq.py b/scipy/cluster/vq.py
index b8da992f4841..87823a663603 100644
--- a/scipy/cluster/vq.py
+++ b/scipy/cluster/vq.py
@@ -783,7 +783,7 @@ def kmeans2(data, k, iter=10, thresh=1e-5, minit='random',
 
     for i in range(iter):
         # Compute the nearest neighbor for each obs using the current code book
-        label = vq(data, code_book)[0]
+        label = vq(data, code_book, check_finite=check_finite)[0]
         # Update the code book by computing centroids
         new_code_book, has_members = _vq.update_cluster_means(data, label, nc)
         if not has_members.all():
"
26,STY: fix mypy and annotations errors,"diff --git a/scipy/stats/_sensitivity_analysis.py b/scipy/stats/_sensitivity_analysis.py
index 553af6960fe0..8aa145d29c48 100644
--- a/scipy/stats/_sensitivity_analysis.py
+++ b/scipy/stats/_sensitivity_analysis.py
@@ -156,8 +156,8 @@ def saltelli_2010(
 
 @dataclass
 class BootstrapSobolResult:
-    first_order: BootstrapResult  # type: ignore[valid-type]
-    total_order: BootstrapResult  # type: ignore[valid-type]
+    first_order: BootstrapResult
+    total_order: BootstrapResult
 
 
 @dataclass
@@ -171,13 +171,13 @@ class SobolResult:
     _A: np.ndarray | None = None
     _B: np.ndarray | None = None
     _AB: np.ndarray | None = None
-    _bootstrap_result: BootstrapResult = None  # type: ignore[valid-type]
+    _bootstrap_result: BootstrapResult | None = None
 
     def bootstrap(
         self,
         confidence_level: DecimalNumber = 0.95,
         n_resamples: IntNumber = 999
-    ) -> BootstrapResult:  # type: ignore[valid-type]
+    ) -> BootstrapSobolResult:
         """"""Bootstrap Sobol' indices to provide confidence intervals.
 
         Parameters
"
27,"DOC: stats.dunnett: document `DunnettResult` attributes (#18215)

DOC: stats.dunnett: document `DunnettResult` attributes","diff --git a/scipy/stats/_multicomp.py b/scipy/stats/_multicomp.py
index f3b276c16590..c12ce65a91db 100644
--- a/scipy/stats/_multicomp.py
+++ b/scipy/stats/_multicomp.py
@@ -25,6 +25,19 @@
 
 @dataclass
 class DunnettResult:
+    """"""Result object returned by `scipy.stats.dunnett`.
+
+    Attributes
+    ----------
+    statistic : float ndarray
+        The computed statistic of the test for each comparison. The element
+        at index ``i`` is the statistic for the comparison between
+        groups ``i`` and the control.
+    pvalue : float ndarray
+        The computed p-value of the test for each comparison. The element
+        at index ``i`` is the p-value for the comparison between
+        group ``i`` and the control.
+    """"""
     statistic: np.ndarray
     pvalue: np.ndarray
     _alternative: Literal['two-sided', 'less', 'greater'] = field(repr=False)
"
29,"DOC: sparse.csgraph: Clarify minimum_spanning_tree behaviour in non-connected case. (#17287)

AFAICT, Kruskal's algorithm (as implemented in _min_spanning_tree.pyx)
works as is for the non-connected case as well: the loop
`while i < n_data and n_edges_in_mst < n_verts - 1` simply keeps going
while never finding an edge that connects separate components, and the
separate components are effectively processed independently.

The behavior is also consistent with networkx's minimum_spanning_edges.","diff --git a/scipy/sparse/csgraph/_min_spanning_tree.pyx b/scipy/sparse/csgraph/_min_spanning_tree.pyx
index 9f6f64a3da33..894f5b6521f0 100644
--- a/scipy/sparse/csgraph/_min_spanning_tree.pyx
+++ b/scipy/sparse/csgraph/_min_spanning_tree.pyx
@@ -50,6 +50,9 @@ def minimum_spanning_tree(csgraph, overwrite=False):
     Small elements < 1E-8 of the dense matrix are rounded to zero.
     All users should input sparse matrices if possible to avoid it.
 
+    If the graph is not connected, this routine returns the minimum spanning
+    forest, i.e. the union of the minimum spanning trees on each connected
+    component.
 
     Examples
     --------
"
31,"DOC: stats.ecdf: correct documentation of `probabilities`, `quantiles`","diff --git a/scipy/stats/_survival.py b/scipy/stats/_survival.py
index ea691e1b0eec..512f486b4479 100644
--- a/scipy/stats/_survival.py
+++ b/scipy/stats/_survival.py
@@ -15,15 +15,16 @@ class EmpiricalDistributionFunction:
 
     Attributes
     ----------
-    q : ndarray
+    quantiles : ndarray
         The unique values of the sample from which the
         `EmpiricalDistributionFunction` was estimated.
-    p : ndarray
+    probabilities : ndarray
         The point estimates of the cumulative distribution function (CDF) or
-        its complement, the survival function (SF), corresponding with `q`.
+        its complement, the survival function (SF), corresponding with
+        `quantiles`.
     """"""
-    q: np.ndarray
-    p: np.ndarray
+    quantiles: np.ndarray
+    probabilities: np.ndarray
     # Exclude these from __str__
     _n: np.ndarray = field(repr=False)  # number ""at risk""
     _d: np.ndarray = field(repr=False)  # number of ""deaths""
@@ -260,19 +261,23 @@ def ecdf(sample):
 
         The `cdf` and `sf` attributes themselves have the following attributes.
 
-        q : ndarray
+        quantiles : ndarray
             The unique values in the sample.
-        p : ndarray
-            The point estimates of the probability corresponding with `q`.
+        probabilities : ndarray
+            The point estimates of the probabilities corresponding with
+            `quantiles`.
 
         And the following methods:
 
-        evaluate(q) :
+        evaluate(x) :
             Evaluate the CDF/SF at the argument.
 
+        plot(ax) :
+            Plot the CDF/SF on the provided axes.
+
         confidence_interval(confidence_level=0.95) :
             Compute the confidence interval around the CDF/SF at the values in
-            `q`.
+            `quantiles`.
 
     Notes
     -----
"
39,"Merge pull request #18207 from dschmitz89/foldednormal_cdf

ENH: improve precision of folded normal distribution cdf","diff --git a/scipy/stats/_continuous_distns.py b/scipy/stats/_continuous_distns.py
index 77efdfb50e2d..1796ae4d672b 100644
--- a/scipy/stats/_continuous_distns.py
+++ b/scipy/stats/_continuous_distns.py
@@ -2348,7 +2348,8 @@ def _pdf(self, x, c):
         return _norm_pdf(x + c) + _norm_pdf(x-c)
 
     def _cdf(self, x, c):
-        return _norm_cdf(x-c) + _norm_cdf(x+c) - 1.0
+        sqrt_two = np.sqrt(2)
+        return 0.5 * (sc.erf((x - c)/sqrt_two) + sc.erf((x + c)/sqrt_two))
 
     def _sf(self, x, c):
         return _norm_sf(x - c) + _norm_sf(x + c)
diff --git a/scipy/stats/tests/test_distributions.py b/scipy/stats/tests/test_distributions.py
index 6035f417c78e..76c0d6892eda 100644
--- a/scipy/stats/tests/test_distributions.py
+++ b/scipy/stats/tests/test_distributions.py
@@ -1122,6 +1122,19 @@ def test_entropy(self, c, ref):
         assert_allclose(stats.gompertz.entropy(c), ref, rtol=1e-14)
 
 
+class TestFoldNorm:
+
+    # reference values were computed with mpmath with 50 digits of precision
+    # from mpmath import mp
+    # mp.dps = 50
+    # mp.mpf(0.5) * (mp.erf((x - c)/mp.sqrt(2)) + mp.erf((x + c)/mp.sqrt(2)))
+
+    @pytest.mark.parametrize('x, c, ref', [(1e-4, 1e-8, 7.978845594730578e-05),
+                                           (1e-4, 1e-4, 7.97884555483635e-05)])
+    def test_cdf(self, x, c, ref):
+        assert_allclose(stats.foldnorm.cdf(x, c), ref, rtol=1e-15)
+
+
 class TestHalfNorm:
 
     # sfx is sf(x).  The values were computed with mpmath:
"
40,TST: test foldnorm CDF precision,"diff --git a/scipy/stats/tests/test_distributions.py b/scipy/stats/tests/test_distributions.py
index 6035f417c78e..76c0d6892eda 100644
--- a/scipy/stats/tests/test_distributions.py
+++ b/scipy/stats/tests/test_distributions.py
@@ -1122,6 +1122,19 @@ def test_entropy(self, c, ref):
         assert_allclose(stats.gompertz.entropy(c), ref, rtol=1e-14)
 
 
+class TestFoldNorm:
+
+    # reference values were computed with mpmath with 50 digits of precision
+    # from mpmath import mp
+    # mp.dps = 50
+    # mp.mpf(0.5) * (mp.erf((x - c)/mp.sqrt(2)) + mp.erf((x + c)/mp.sqrt(2)))
+
+    @pytest.mark.parametrize('x, c, ref', [(1e-4, 1e-8, 7.978845594730578e-05),
+                                           (1e-4, 1e-4, 7.97884555483635e-05)])
+    def test_cdf(self, x, c, ref):
+        assert_allclose(stats.foldnorm.cdf(x, c), ref, rtol=1e-15)
+
+
 class TestHalfNorm:
 
     # sfx is sf(x).  The values were computed with mpmath:
"
41,ENH: improve foldnormal precision,"diff --git a/scipy/stats/_continuous_distns.py b/scipy/stats/_continuous_distns.py
index 77efdfb50e2d..1796ae4d672b 100644
--- a/scipy/stats/_continuous_distns.py
+++ b/scipy/stats/_continuous_distns.py
@@ -2348,7 +2348,8 @@ def _pdf(self, x, c):
         return _norm_pdf(x + c) + _norm_pdf(x-c)
 
     def _cdf(self, x, c):
-        return _norm_cdf(x-c) + _norm_cdf(x+c) - 1.0
+        sqrt_two = np.sqrt(2)
+        return 0.5 * (sc.erf((x - c)/sqrt_two) + sc.erf((x + c)/sqrt_two))
 
     def _sf(self, x, c):
         return _norm_sf(x - c) + _norm_sf(x + c)
"
43,"BUG: sparse: Use _ascontainer for argmin/argmax (#18121)

* BUG: sparse: Use _ascontainer for argmin/argmax

* Adding test coverage

* Remove unused import

* skip sparse formats that don't support min/max","diff --git a/scipy/sparse/_data.py b/scipy/sparse/_data.py
index e977f18517f5..68f855d82dba 100644
--- a/scipy/sparse/_data.py
+++ b/scipy/sparse/_data.py
@@ -9,7 +9,7 @@
 import numpy as np
 
 from ._base import spmatrix, _ufuncs_with_fixed_point_at_zero
-from ._sputils import isscalarlike, validateaxis, matrix
+from ._sputils import isscalarlike, validateaxis
 
 __all__ = []
 
@@ -250,7 +250,7 @@ def _arg_min_or_max_axis(self, axis, op, compare):
         if axis == 1:
             ret = ret.reshape(-1, 1)
 
-        return matrix(ret)
+        return self._ascontainer(ret)
 
     def _arg_min_or_max(self, axis, out, op, compare):
         if out is not None:
diff --git a/scipy/sparse/tests/test_array_api.py b/scipy/sparse/tests/test_array_api.py
index a074cae7759d..45b16b350118 100644
--- a/scipy/sparse/tests/test_array_api.py
+++ b/scipy/sparse/tests/test_array_api.py
@@ -58,6 +58,23 @@ def test_mean(A):
         ""Expected array, got matrix""
 
 
+@parametrize_sparrays
+def test_min_max(A):
+    # Some formats don't support min/max operations, so we skip those here.
+    if hasattr(A, 'min'):
+        assert not isinstance(A.min(axis=1), np.matrix), \
+            ""Expected array, got matrix""
+    if hasattr(A, 'max'):
+        assert not isinstance(A.max(axis=1), np.matrix), \
+            ""Expected array, got matrix""
+    if hasattr(A, 'argmin'):
+        assert not isinstance(A.argmin(axis=1), np.matrix), \
+            ""Expected array, got matrix""
+    if hasattr(A, 'argmax'):
+        assert not isinstance(A.argmax(axis=1), np.matrix), \
+            ""Expected array, got matrix""
+
+
 @parametrize_sparrays
 def test_todense(A):
     assert not isinstance(A.todense(), np.matrix), \
"
45,"MAINT,DOC: Update dependencies and warnings to ignore when building docs","diff --git a/dev.py b/dev.py
index 57b323f98db9..97970fb11b4f 100644
--- a/dev.py
+++ b/dev.py
@@ -987,6 +987,10 @@ def task_meta(cls, list_targets, parallel, args, **kwargs):
         make_params = [f'PYTHON=""{sys.executable}""']
         if parallel:
             make_params.append(f'SPHINXOPTS=""-j{parallel}""')
+        # Environment variables needed for notebooks
+        
+        make_params.append('SQLALCHEMY_SILENCE_UBER_WARNING=1')
+        make_params.append('JUPYTER_PLATFORM_DIRS=1')
 
         return {
             'actions': [
diff --git a/doc/source/conf.py b/doc/source/conf.py
index a14291a47457..d2afdeecbd1d 100644
--- a/doc/source/conf.py
+++ b/doc/source/conf.py
@@ -156,6 +156,12 @@
 warnings.filterwarnings(  # matplotlib<->pyparsing issue
     'ignore', message=""Exception creating Regex for oneOf.*"",
     category=SyntaxWarning)
+warnings.filterwarnings(  # docutils warning when using notebooks
+    'ignore', message=""The frontend.OptionParser class will be replaced"",
+    category=DeprecationWarning)
+warnings.filterwarnings(
+    'ignore', message=""The frontend.Option class will be removed in Docutils 0.21 or later."",
+    category=DeprecationWarning)
 # warnings in examples (mostly) that we allow
 # TODO: eventually these should be eliminated!
 for key in (
diff --git a/environment.yml b/environment.yml
index 73bf588f9d7e..122efccffda0 100644
--- a/environment.yml
+++ b/environment.yml
@@ -38,7 +38,6 @@ dependencies:
   - matplotlib
   - pydata-sphinx-theme==0.9.0
   - sphinx-design
-  - docutils<0.18.1
   - jupytext
   - myst-nb
   # For linting
"
51,DOC: Optimize: Fix for side bar rendering on top of Hessian (#18189),"diff --git a/doc/source/tutorial/optimize.rst b/doc/source/tutorial/optimize.rst
index f823480b5a61..1319cb5fe9d2 100644
--- a/doc/source/tutorial/optimize.rst
+++ b/doc/source/tutorial/optimize.rst
@@ -288,7 +288,7 @@ For example, the Hessian when :math:`N=5` is
 
 .. math::
 
-    \mathbf{H}=\begin{bmatrix} 1200x_{0}^{2}-400x_{1}+2 & -400x_{0} & 0 & 0 & 0\\ -400x_{0} & 202+1200x_{1}^{2}-400x_{2} & -400x_{1} & 0 & 0\\ 0 & -400x_{1} & 202+1200x_{2}^{2}-400x_{3} & -400x_{2} & 0\\ 0 &  & -400x_{2} & 202+1200x_{3}^{2}-400x_{4} & -400x_{3}\\ 0 & 0 & 0 & -400x_{3} & 200\end{bmatrix}.
+    \mathbf{H}=\begin{bmatrix} 1200x_{0}^{2}+2\mkern-2em\\&1200x_{1}^{2}+202\mkern-2em\\&&1200x_{1}^{2}+202\mkern-2em\\&&&1200x_{3}^{2}+202\mkern-1em\\&&&&200\end{bmatrix}-400\begin{bmatrix} x_1 & x_0 \\ x_0 & x_2 & x_1 \\ & x_1 & x_3 & x_2\\ & & x_2 & x_4 & x_3 \\ & & & x_3 & 0\end{bmatrix}.
 
 The code which computes this Hessian along with the code to minimize
 the function using Newton-CG method is shown in the following example:
"
56,BUG: interpolate: add x-y length validation for `make_smoothing_spline`. (#18188),"diff --git a/scipy/interpolate/_bsplines.py b/scipy/interpolate/_bsplines.py
index dca42ad5c4ae..13efb49f803f 100644
--- a/scipy/interpolate/_bsplines.py
+++ b/scipy/interpolate/_bsplines.py
@@ -1877,9 +1877,9 @@ def make_smoothing_spline(x, y, w=None, lam=None):
     Parameters
     ----------
     x : array_like, shape (n,)
-        Abscissas.
+        Abscissas. `n` must be larger than 5.
     y : array_like, shape (n,)
-        Ordinates.
+        Ordinates. `n` must be larger than 5.
     w : array_like, shape (n,), optional
         Vector of weights. Default is ``np.ones_like(x)``.
     lam : float, (:math:`\lambda \geq 0`), optional
@@ -1980,6 +1980,9 @@ def make_smoothing_spline(x, y, w=None, lam=None):
     t = np.r_[[x[0]] * 3, x, [x[-1]] * 3]
     n = x.shape[0]
 
+    if n <= 4:
+        raise ValueError('``x`` and ``y`` length must be larger than 5')
+
     # It is known that the solution to the stated minimization problem exists
     # and is a natural cubic spline with vector of knots equal to the unique
     # elements of ``x`` [3], so we will solve the problem in the basis of
diff --git a/scipy/interpolate/tests/test_bsplines.py b/scipy/interpolate/tests/test_bsplines.py
index 9bcd97e2e222..b411bbdcfc8d 100644
--- a/scipy/interpolate/tests/test_bsplines.py
+++ b/scipy/interpolate/tests/test_bsplines.py
@@ -1560,6 +1560,13 @@ def test_invalid_input(self):
         with assert_raises(ValueError):
             make_smoothing_spline(x_dupl, y)
 
+        # x and y length must be larger than 5
+        x = np.arange(4)
+        y = np.ones(4)
+        exception_message = ""``x`` and ``y`` length must be larger than 5""
+        with pytest.raises(ValueError, match=exception_message):
+            make_smoothing_spline(x, y)
+
     def test_compare_with_GCVSPL(self):
         """"""
         Data is generated in the following way:
"
57,"ENH: Added `_sf` method for anglit distribution (#17832) (#18178)

* ENH: Added `_sf` method for anglit distribution (#17832)

Co-authored-by: Matt Haberland <mhaberla@calpoly.edu>","diff --git a/scipy/stats/_continuous_distns.py b/scipy/stats/_continuous_distns.py
index ef3155542494..0e19e0d39e4b 100644
--- a/scipy/stats/_continuous_distns.py
+++ b/scipy/stats/_continuous_distns.py
@@ -529,6 +529,9 @@ def _pdf(self, x):
     def _cdf(self, x):
         return np.sin(x+np.pi/4)**2.0
 
+    def _sf(self, x):
+        return np.cos(x + np.pi / 4) ** 2.0
+
     def _ppf(self, q):
         return np.arcsin(np.sqrt(q))-np.pi/4
 
"
58,"DOC: Fixed missing curly bracket in scipy.css

[skip cirrus] [skip azp]","diff --git a/doc/source/_static/scipy.css b/doc/source/_static/scipy.css
index ff863f96e313..70bf83c07607 100644
--- a/doc/source/_static/scipy.css
+++ b/doc/source/_static/scipy.css
@@ -169,6 +169,7 @@ html[data-theme=dark] .sd-card .sd-card-footer {
 .sd-dropdown .sd-card-header {
   padding: 0px 0px 0px 0px;
   text-align: left;
+}
 
 /* Legacy admonition */
 
"
60,DOC: Move legacy directive to not be first in the file,"diff --git a/doc/source/dev/contributor/rendering_documentation.rst b/doc/source/dev/contributor/rendering_documentation.rst
index c87da990f8cb..c026a7a68e3a 100644
--- a/doc/source/dev/contributor/rendering_documentation.rst
+++ b/doc/source/dev/contributor/rendering_documentation.rst
@@ -322,6 +322,7 @@ This will create the following output:
 
 .. legacy:: function
 
+---
 
 .. _GitHub: https://github.com/
 .. _CircleCI: https://circleci.com/vcs-authorize/
diff --git a/scipy/interpolate/_interpolate.py b/scipy/interpolate/_interpolate.py
index cd2c6aa8521b..0e81df38b056 100644
--- a/scipy/interpolate/_interpolate.py
+++ b/scipy/interpolate/_interpolate.py
@@ -383,10 +383,10 @@ def _do_extrapolate(fill_value):
 
 class interp1d(_Interpolator1D):
     """"""
-    .. legacy:: function
-
     Interpolate a 1-D function.
 
+    .. legacy:: function
+
     `x` and `y` are arrays of values used to approximate some function f:
     ``y = f(x)``. This class returns a function whose call method uses
     interpolation to find the value of new points.
"
62,DOC: Add optional argument to Legacy directive,"diff --git a/doc/source/conf.py b/doc/source/conf.py
index 0cf3beb60705..494d013e49aa 100644
--- a/doc/source/conf.py
+++ b/doc/source/conf.py
@@ -433,7 +433,12 @@ class LegacyDirective(Directive):
     node_class = nodes.admonition
 
     def run(self):
-        text = (""This submodule is now considered legacy and will no longer ""
+        try:
+            obj = self.arguments[0]
+        except IndexError:
+            # Argument is empty; use default text
+            obj = ""submodule""
+        text = (f""This {obj} is now considered legacy and will no longer ""
                 ""receive updates."")
 
         try:
diff --git a/scipy/interpolate/_interpolate.py b/scipy/interpolate/_interpolate.py
index 9349c8a6bcc9..cd2c6aa8521b 100644
--- a/scipy/interpolate/_interpolate.py
+++ b/scipy/interpolate/_interpolate.py
@@ -383,6 +383,8 @@ def _do_extrapolate(fill_value):
 
 class interp1d(_Interpolator1D):
     """"""
+    .. legacy:: function
+
     Interpolate a 1-D function.
 
     `x` and `y` are arrays of values used to approximate some function f:
"
63,"DOC: Ignore legacy directive in refguide_check

[skip actions]","diff --git a/doc/source/conf.py b/doc/source/conf.py
index 32a750298d6b..0cf3beb60705 100644
--- a/doc/source/conf.py
+++ b/doc/source/conf.py
@@ -440,7 +440,9 @@ def run(self):
             self.content[0] = text+"" ""+self.content[0]
         except IndexError:
             # Content is empty; use the default text
-            source, lineno = self.state_machine.get_source_and_line(self.lineno)
+            source, lineno = self.state_machine.get_source_and_line(
+                self.lineno
+            )
             self.content.append(
                 text,
                 source=source,
@@ -460,5 +462,6 @@ def run(self):
                                 admonition_node)
         return [admonition_node]
 
+
 def setup(app):
     app.add_directive(""legacy"", LegacyDirective)
diff --git a/tools/refguide_check.py b/tools/refguide_check.py
index 237806c97a11..764c32fca610 100755
--- a/tools/refguide_check.py
+++ b/tools/refguide_check.py
@@ -52,7 +52,6 @@
 directives.register_directive('seealso', SeeAlso)
 directives.register_directive('only', Only)
 
-
 BASE_MODULE = ""scipy""
 
 PUBLIC_SUBMODULES = [
@@ -341,7 +340,7 @@ def validate_rst_syntax(text, name, dots=True):
         return False, ""ERROR: %s: no documentation"" % (name,)
 
     ok_unknown_items = set([
-        'mod', 'currentmodule', 'autosummary', 'data',
+        'mod', 'currentmodule', 'autosummary', 'data', 'legacy',
         'obj', 'versionadded', 'versionchanged', 'module', 'class', 'meth',
         'ref', 'func', 'toctree', 'moduleauthor', 'deprecated',
         'sectionauthor', 'codeauthor', 'eq', 'doi', 'DOI', 'arXiv', 'arxiv'
"
64,DOC: Documenting the usage of the legacy directive,"diff --git a/doc/source/dev/contributor/rendering_documentation.rst b/doc/source/dev/contributor/rendering_documentation.rst
index 7a840e7f7263..4e52110de534 100644
--- a/doc/source/dev/contributor/rendering_documentation.rst
+++ b/doc/source/dev/contributor/rendering_documentation.rst
@@ -286,6 +286,32 @@ the use of a seed in their program. The consequence is that users cannot
 reproduce the results of the example exactly, so examples using random data
 should not refer to precise numerical values based on random data or rely on
 them to make their point.
+Legacy directive
+~~~~~~~~~~~~~~~~
+
+If a function, module or API is in *legacy* mode, meaning that it is kept around
+for backwards compatibility reasons, but is not recommended to use in new code,
+you can use the ``.. legacy::`` directive.
+
+By default, if used with no arguments, the legacy directive will generate the
+following output:
+
+.. legacy::
+
+
+If you need to include a custom message, such as a new API to replace the old
+one, this message will be appended to the default message::
+
+   .. legacy::
+
+      New code should use :mod:`scipy.fft`.
+
+will create the following output:
+
+.. legacy::
+
+   New code should use :mod:`scipy.fft`.
+
 
 .. _GitHub: https://github.com/
 .. _CircleCI: https://circleci.com/vcs-authorize/
"
66,MAINT: stats.ecdf: store number at risk just before events (#18187),"diff --git a/scipy/stats/_survival.py b/scipy/stats/_survival.py
index 33eda7ffe265..ecfa933022ad 100644
--- a/scipy/stats/_survival.py
+++ b/scipy/stats/_survival.py
@@ -325,12 +325,15 @@ def _ecdf_uncensored(sample):
     x, counts = np.unique(sample, return_counts=True)
 
     # [1].81 ""the fraction of [observations] that are less than or equal to x
-    cdf = np.cumsum(counts) / sample.size
+    events = np.cumsum(counts)
+    n = sample.size
+    cdf = events / n
 
     # [1].89 ""the relative frequency of the sample that exceeds x in value""
     sf = 1 - cdf
 
-    return x, cdf, sf, sf * sample.size, counts
+    at_risk = np.concatenate(([n], n - events[:-1]))
+    return x, cdf, sf, at_risk, counts
 
 
 def _ecdf_right_censored(sample):
diff --git a/scipy/stats/tests/test_survival.py b/scipy/stats/tests/test_survival.py
index 0f614246e668..3120c8f3ea79 100644
--- a/scipy/stats/tests/test_survival.py
+++ b/scipy/stats/tests/test_survival.py
@@ -325,3 +325,15 @@ def test_right_censored_ci_nans(self):
                 0.8263946341076415, 0.6558775085110887, np.nan]
         assert_allclose(ci.low, low)
         assert_allclose(ci.high, high)
+
+    def test_right_censored_against_uncensored(self):
+        rng = np.random.default_rng(7463952748044886637)
+        sample = rng.integers(10, 100, size=1000)
+        censored = np.zeros_like(sample)
+        censored[np.argmax(sample)] = True
+        res = stats.ecdf(sample)
+        ref = stats.ecdf(stats.CensoredData.right_censored(sample, censored))
+        assert_equal(res.sf._x, ref.sf._x)
+        assert_equal(res.sf._n, ref.sf._n)
+        assert_equal(res.sf._d[:-1], ref.sf._d[:-1])  # difference @ [-1]
+        assert_allclose(res.sf._sf[:-1], ref.sf._sf[:-1], rtol=1e-14)
"
67,"DOC: cite pip issue about multiple `--config-settings` (#18174)

Allows readers to follow up to see if the pip enhancement is available.
As of today the pip PR is merged, but not released.

[skip ci]

Co-authored-by: Ralf Gommers <ralf.gommers@gmail.com>","diff --git a/doc/source/dev/contributor/meson_advanced.rst b/doc/source/dev/contributor/meson_advanced.rst
index 67a61f5466fc..78a571346ea0 100644
--- a/doc/source/dev/contributor/meson_advanced.rst
+++ b/doc/source/dev/contributor/meson_advanced.rst
@@ -26,10 +26,12 @@ implementations on conda-forge), use::
 
 Other options that should work (as long as they're installed with
 ``pkg-config`` or CMake support) include ``mkl`` and ``blis``. Note that using
-``pip install`` or ``pip wheel`` doesn't work (as of Jan'23) because we need
-two ``setup-args`` flags for specifying both ``blas`` and ``lapack`` here, and
-``pip`` does not yet support specifying ``--config-settings`` with the same key
-twice, while ``build`` does support that.
+``pip install`` or ``pip wheel`` doesn't work (pending release of the fix for
+`pip#11681 <https://github.com/pypa/pip/issues/11681>`__, likely in Pip
+23.1.0) because we need two ``setup-args`` flags for specifying both ``blas``
+and ``lapack`` here, and ``pip`` does not yet support specifying
+``--config-settings`` with the same key twice, while ``build`` does support
+that.
 
 .. note::
 
"
68,"DOC: update links for ARPACK to point to ARPACK-NG (#18173)

[skip ci]","diff --git a/doc/source/tutorial/arpack.rst b/doc/source/tutorial/arpack.rst
index 5932c3dcd62f..5b3ed7adc67c 100644
--- a/doc/source/tutorial/arpack.rst
+++ b/doc/source/tutorial/arpack.rst
@@ -307,4 +307,4 @@ operator.
 
 References
 ----------
-.. [1] http://www.caam.rice.edu/software/ARPACK/
+.. [1] https://github.com/opencollab/arpack-ng
diff --git a/scipy/sparse/linalg/_eigen/arpack/arpack.py b/scipy/sparse/linalg/_eigen/arpack/arpack.py
index 7f0bd2f10211..1221d50daba9 100644
--- a/scipy/sparse/linalg/_eigen/arpack/arpack.py
+++ b/scipy/sparse/linalg/_eigen/arpack/arpack.py
@@ -2,7 +2,7 @@
 Find a few eigenvectors and eigenvalues of a matrix.
 
 
-Uses ARPACK: http://www.caam.rice.edu/software/ARPACK/
+Uses ARPACK: https://github.com/opencollab/arpack-ng
 
 """"""
 # Wrapper implementation notes
@@ -1234,7 +1234,7 @@ def eigs(A, k=6, M=None, sigma=None, which='LM', v0=None,
 
     References
     ----------
-    .. [1] ARPACK Software, http://www.caam.rice.edu/software/ARPACK/
+    .. [1] ARPACK Software, https://github.com/opencollab/arpack-ng
     .. [2] R. B. Lehoucq, D. C. Sorensen, and C. Yang,  ARPACK USERS GUIDE:
        Solution of Large Scale Eigenvalue Problems by Implicitly Restarted
        Arnoldi Methods. SIAM, Philadelphia, PA, 1998.
@@ -1543,7 +1543,7 @@ def eigsh(A, k=6, M=None, sigma=None, which='LM', v0=None,
 
     References
     ----------
-    .. [1] ARPACK Software, http://www.caam.rice.edu/software/ARPACK/
+    .. [1] ARPACK Software, https://github.com/opencollab/arpack-ng
     .. [2] R. B. Lehoucq, D. C. Sorensen, and C. Yang,  ARPACK USERS GUIDE:
        Solution of Large Scale Eigenvalue Problems by Implicitly Restarted
        Arnoldi Methods. SIAM, Philadelphia, PA, 1998.
"
70,MAINT: stats.logistic.fit: simplify,"diff --git a/scipy/stats/_continuous_distns.py b/scipy/stats/_continuous_distns.py
index 3e9d3112610c..599193e13e48 100644
--- a/scipy/stats/_continuous_distns.py
+++ b/scipy/stats/_continuous_distns.py
@@ -5787,19 +5787,13 @@ def fit(self, data, *args, **kwds):
         # scale parameters are roots of the two equations described in `func`.
         # Source: Statistical Distributions, 3rd Edition. Evans, Hastings, and
         # Peacock (2000), Page 130
-        # Note: gh-18176 reported data for which the reported MLE had
-        # `scale < 0`. To fix the bug, we'll use abs(scale). This is not
-        # expected to cause convergence issues because a) `dl_dscale` is an
-        # even function, b) dl_dloc appears to be approximately even at the
-        # solution, and c) in 10k randomly-generated tests, the fit override
-        # always outperformed the generic fit. Ideally, though, we would
-        # *constrain* `scale` to be positive.
+
         def dl_dloc(loc, scale=fscale):
-            c = (data - loc) / abs(scale)
+            c = (data - loc) / scale
             return np.sum(sc.expit(c)) - n/2
 
         def dl_dscale(scale, loc=floc):
-            c = (data - loc) / abs(scale)
+            c = (data - loc) / scale
             return np.sum(c*np.tanh(c/2)) - n
 
         def func(params):
@@ -5818,6 +5812,10 @@ def func(params):
             res = optimize.root(func, (loc, scale))
             loc, scale = res.x
 
+        # Note: gh-18176 reported data for which the reported MLE had
+        # `scale < 0`. To fix the bug, we return abs(scale). This is OK because
+        # `dl_dscale` and `dl_dloc` are even and odd functions of `scale`,
+        # respectively, so if `-scale` is a solution, so is `scale`.
         scale = abs(scale)
         return ((loc, scale) if res.success
                 else super().fit(data, *args, **kwds))
"
74,"MAINT: method as keyword only argument in CI. [skip ci]

Co-authored-by: Matt Haberland <mhaberla@calpoly.edu>","diff --git a/scipy/stats/_survival.py b/scipy/stats/_survival.py
index d02308f66d6b..33eda7ffe265 100644
--- a/scipy/stats/_survival.py
+++ b/scipy/stats/_survival.py
@@ -35,7 +35,7 @@ def __init__(self, x, points, n, d, kind):
         self._sf = points if kind == 'sf' else 1 - points
         self._kind = kind
 
-    def confidence_interval(self, confidence_level=0.95, method='linear'):
+    def confidence_interval(self, confidence_level=0.95, *, method='linear'):
         """"""Compute a confidence interval around the CDF/SF point estimate
 
         Parameters
"
79,"TST: stats.mstats.trimmed_var/std: add tests (#18019)

* add: tests for trimmed_var
* add: tests for trimmed_std","diff --git a/scipy/stats/tests/test_mstats_basic.py b/scipy/stats/tests/test_mstats_basic.py
index 6b00ddb1361e..1cab645b81c3 100644
--- a/scipy/stats/tests/test_mstats_basic.py
+++ b/scipy/stats/tests/test_mstats_basic.py
@@ -550,6 +550,26 @@ def test_trimmedmean(self):
         assert_almost_equal(mstats.trimmed_mean(data,(0.1,0.1)), 343, 0)
         assert_almost_equal(mstats.trimmed_mean(data,(0.2,0.2)), 283, 0)
 
+    def test_trimmedvar(self):
+        # Basic test. Additional tests of all arguments, edge cases,
+        # input validation, and proper treatment of masked arrays are needed.
+        rng = np.random.default_rng(3262323289434724460)
+        data_orig = rng.random(size=20)
+        data = np.sort(data_orig)
+        data = ma.array(data, mask=[1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
+                                    0, 0, 0, 0, 0, 0, 0, 0, 1, 1])
+        assert_allclose(mstats.trimmed_var(data_orig, 0.1), data.var())
+
+    def test_trimmedstd(self):
+        # Basic test. Additional tests of all arguments, edge cases,
+        # input validation, and proper treatment of masked arrays are needed.
+        rng = np.random.default_rng(7121029245207162780)
+        data_orig = rng.random(size=20)
+        data = np.sort(data_orig)
+        data = ma.array(data, mask=[1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
+                                    0, 0, 0, 0, 0, 0, 0, 0, 1, 1])
+        assert_allclose(mstats.trimmed_std(data_orig, 0.1), data.std())
+
     def test_trimmed_stde(self):
         data = ma.array([77, 87, 88,114,151,210,219,246,253,262,
                          296,299,306,376,428,515,666,1310,2611])
"
80,"TST: spatial: skip failing test to make CI green again (#18168)

* skip test failures related to gh-18108

---------

Co-authored-by: Tyler Reddy <tyler.je.reddy@gmail.com>","diff --git a/scipy/spatial/tests/test_distance.py b/scipy/spatial/tests/test_distance.py
index 762f1060cadb..227db38e8910 100644
--- a/scipy/spatial/tests/test_distance.py
+++ b/scipy/spatial/tests/test_distance.py
@@ -568,6 +568,7 @@ def test_cdist_dtype_equivalence(self):
                         y2 = cdist(new_type(X1), new_type(X2), metric=metric)
                         assert_allclose(y1, y2, rtol=eps, verbose=verbose > 2)
 
+    @pytest.mark.skip(""Failing on Windows Azure jobs; see gh-18108."")
     def test_cdist_out(self):
         # Test that out parameter works properly
         eps = 1e-15
@@ -1468,6 +1469,7 @@ def test_pdist_dtype_equivalence(self):
                         y2 = pdist(new_type(X1), metric=metric)
                         assert_allclose(y1, y2, rtol=eps, verbose=verbose > 2)
 
+    @pytest.mark.skip(""Failing on Windows Azure jobs; see gh-18108."")
     def test_pdist_out(self):
         # Test that out parameter works properly
         eps = 1e-15
@@ -2071,6 +2073,7 @@ def test_Xdist_deprecated_args():
                 pdist(X1, metric, **kwargs)
 
 
+@pytest.mark.skip(""Failing on Windows Azure jobs; see gh-18108."")
 def test_Xdist_non_negative_weights():
     X = eo['random-float32-data'][::5, ::2]
     w = np.ones(X.shape[1])
"
88,"Merge pull request #18159 from OmarManzoor/const_memory_views_bspl

FIX: const with double_or_complex memory views in _bspl.pyx","diff --git a/scipy/interpolate/_bspl.pyx b/scipy/interpolate/_bspl.pyx
index 5faf3a710f95..a19ec0644a47 100644
--- a/scipy/interpolate/_bspl.pyx
+++ b/scipy/interpolate/_bspl.pyx
@@ -338,7 +338,7 @@ def _handle_lhs_derivatives(const double[::1]t, int k, double xval,
 def _norm_eq_lsq(const double[::1] x,
                  const double[::1] t,
                  int k,
-                 double_or_complex[:, ::1] y,
+                 const double_or_complex[:, ::1] y,
                  const double[::1] w,
                  double[::1, :] ab,
                  double_or_complex[::1, :] rhs):
diff --git a/scipy/interpolate/tests/test_bsplines.py b/scipy/interpolate/tests/test_bsplines.py
index 4ee648fe7b8e..9bcd97e2e222 100644
--- a/scipy/interpolate/tests/test_bsplines.py
+++ b/scipy/interpolate/tests/test_bsplines.py
@@ -1519,6 +1519,13 @@ def test_checkfinite(self):
             y[-1] = z
             assert_raises(ValueError, make_lsq_spline, x, y, t)
 
+    def test_read_only(self):
+        # Check that make_lsq_spline works with read only arrays
+        x, y, t = self.x, self.y, self.t
+        x.setflags(write=False)
+        y.setflags(write=False)
+        t.setflags(write=False)
+        make_lsq_spline(x=x, y=y, t=t)
 
 def data_file(basename):
     return os.path.join(os.path.abspath(os.path.dirname(__file__)),
"
90,MAINT: add test against generic fit method for vonmises,"diff --git a/scipy/stats/tests/test_distributions.py b/scipy/stats/tests/test_distributions.py
index fc4fb9ee1959..230782ae9dc1 100644
--- a/scipy/stats/tests/test_distributions.py
+++ b/scipy/stats/tests/test_distributions.py
@@ -198,6 +198,28 @@ def test_vonmises_expect(self):
         assert_allclose(np.angle(res), loc % (2*np.pi))
         assert np.issubdtype(res.dtype, np.complexfloating)
 
+    @pytest.mark.parametrize(""rvs_loc"", [0, 2])
+    @pytest.mark.parametrize(""rvs_scale"", [1, 100])
+    @pytest.mark.parametrize('fix_loc', [True, False])
+    def test_fit_MLE_comp_optimzer(self, rvs_loc, rvs_scale,
+                                   fix_loc):
+
+        rng = np.random.default_rng(6762668991392531563)
+        data = stats.vonmises.rvs(rvs_scale, size=1000, loc=rvs_loc,
+                                  random_state=rng)
+
+        def negative_loglikelihood(fit_result, data):
+            kappa, loc, scale = fit_result
+            logpdf = stats.vonmises(kappa, loc=loc).logpdf(data).sum()
+            return -logpdf
+
+        kwds = {}
+        if fix_loc:
+            kwds['floc'] = rvs_loc
+
+        _assert_less_or_close_loglike(stats.vonmises, data,
+                                      negative_loglikelihood, **kwds)
+
     @pytest.mark.parametrize('loc', [-0.5 * np.pi, 0, np.pi])
     @pytest.mark.parametrize('kappa', [1, 10, 100, 1000])
     def test_vonmises_fit_all(self, kappa, loc):
"
92,"ENH: stats.dweibull.entropy: implement with weibull_min.entropy (#18162)

* ENH: stats.dweibull.entropy: implement with weibull_min.entropy

Co-authored-by: Matt Haberland <mhaberla@calpoly.edu>","diff --git a/scipy/stats/_continuous_distns.py b/scipy/stats/_continuous_distns.py
index 6d653fd17ce1..106c158fc34e 100644
--- a/scipy/stats/_continuous_distns.py
+++ b/scipy/stats/_continuous_distns.py
@@ -1738,6 +1738,10 @@ def _munp(self, n, c):
     def _stats(self, c):
         return 0, None, 0, None
 
+    def _entropy(self, c):
+        h = stats.weibull_min._entropy(c) - np.log(0.5)
+        return h
+
 
 dweibull = dweibull_gen(name='dweibull')
 
diff --git a/scipy/stats/tests/test_distributions.py b/scipy/stats/tests/test_distributions.py
index bd4e4b8c4b44..f893da681548 100644
--- a/scipy/stats/tests/test_distributions.py
+++ b/scipy/stats/tests/test_distributions.py
@@ -6143,6 +6143,21 @@ def test_fit_min(self):
         assert_allclose(res, ref)
 
 
+class TestDweibull:
+    def test_entropy(self):
+        # Test that dweibull entropy follows that of weibull_min.
+        # (Generic tests check that the dweibull entropy is consistent
+        #  with its PDF. As for accuracy, dweibull entropy should be just
+        #  as accurate as weibull_min entropy. Checks of accuracy against
+        #  a reference need only be applied to the fundamental distribution -
+        #  weibull_min.)
+        rng = np.random.default_rng(8486259129157041777)
+        c = 10**rng.normal(scale=100, size=10)
+        res = stats.dweibull.entropy(c)
+        ref = stats.weibull_min.entropy(c) - np.log(0.5)
+        assert_allclose(res, ref, rtol=1e-15)
+
+
 class TestTruncWeibull:
 
     def test_pdf_bounds(self):
"
93,"Merge pull request #18158 from MatteoRaso/levy_doc

DOC: stats: technically, expressions for levy and levy_l PDFs are valid only for x > 0","diff --git a/scipy/stats/_continuous_distns.py b/scipy/stats/_continuous_distns.py
index 0bef50a409fc..6d653fd17ce1 100644
--- a/scipy/stats/_continuous_distns.py
+++ b/scipy/stats/_continuous_distns.py
@@ -5428,7 +5428,7 @@ class levy_gen(rv_continuous):
 
         f(x) = \frac{1}{\sqrt{2\pi x^3}} \exp\left(-\frac{1}{2x}\right)
 
-    for :math:`x >= 0`.
+    for :math:`x > 0`.
 
     This is the same as the Levy-stable distribution with :math:`a=1/2` and
     :math:`b=1`.
@@ -5531,7 +5531,7 @@ class levy_l_gen(rv_continuous):
     .. math::
         f(x) = \frac{1}{|x| \sqrt{2\pi |x|}} \exp{ \left(-\frac{1}{2|x|} \right)}
 
-    for :math:`x <= 0`.
+    for :math:`x < 0`.
 
     This is the same as the Levy-stable distribution with :math:`a=1/2` and
     :math:`b=-1`.
"
94,"CI: fix pre-release job that is failing on Cython 3.0b1 (#18157)

[skip ci]

Co-authored-by: Tyler Reddy <tyler.je.reddy@gmail.com>","diff --git a/ci/azure-travis-template.yaml b/ci/azure-travis-template.yaml
index 39a448f1f3d8..034027da22b0 100644
--- a/ci/azure-travis-template.yaml
+++ b/ci/azure-travis-template.yaml
@@ -70,7 +70,7 @@ steps:
     pip install --upgrade ${{parameters.numpy_spec}} &&
     pip install --upgrade pip setuptools==59.6.0 wheel build meson meson-python &&
     pip install ${{parameters.other_spec}}
-    cython
+    ""cython<3.0b1""
     gmpy2
     threadpoolctl
     mpmath
"
95,FIX: const with double_or_complex memory views in _bspl.pyx,"diff --git a/scipy/interpolate/_bspl.pyx b/scipy/interpolate/_bspl.pyx
index 5faf3a710f95..a19ec0644a47 100644
--- a/scipy/interpolate/_bspl.pyx
+++ b/scipy/interpolate/_bspl.pyx
@@ -338,7 +338,7 @@ def _handle_lhs_derivatives(const double[::1]t, int k, double xval,
 def _norm_eq_lsq(const double[::1] x,
                  const double[::1] t,
                  int k,
-                 double_or_complex[:, ::1] y,
+                 const double_or_complex[:, ::1] y,
                  const double[::1] w,
                  double[::1, :] ab,
                  double_or_complex[::1, :] rhs):
diff --git a/scipy/interpolate/tests/test_bsplines.py b/scipy/interpolate/tests/test_bsplines.py
index 4ee648fe7b8e..9bcd97e2e222 100644
--- a/scipy/interpolate/tests/test_bsplines.py
+++ b/scipy/interpolate/tests/test_bsplines.py
@@ -1519,6 +1519,13 @@ def test_checkfinite(self):
             y[-1] = z
             assert_raises(ValueError, make_lsq_spline, x, y, t)
 
+    def test_read_only(self):
+        # Check that make_lsq_spline works with read only arrays
+        x, y, t = self.x, self.y, self.t
+        x.setflags(write=False)
+        y.setflags(write=False)
+        t.setflags(write=False)
+        make_lsq_spline(x=x, y=y, t=t)
 
 def data_file(basename):
     return os.path.join(os.path.abspath(os.path.dirname(__file__)),
"
96,"FIX: add const with fused type memory view in evaluate_spline (#18153)

* FIX: add const with fused type memory view in evaluate_spline

* Add comment in test","diff --git a/scipy/interpolate/_bspl.pyx b/scipy/interpolate/_bspl.pyx
index db544c1c44f2..5faf3a710f95 100644
--- a/scipy/interpolate/_bspl.pyx
+++ b/scipy/interpolate/_bspl.pyx
@@ -91,7 +91,7 @@ cdef inline int find_interval(const double[::1] t,
 @cython.boundscheck(False)
 @cython.cdivision(True)
 def evaluate_spline(const double[::1] t,
-             double_or_complex[:, ::1] c,
+             const double_or_complex[:, ::1] c,
              int k,
              const double[::1] xp,
              int nu,
diff --git a/scipy/interpolate/tests/test_bsplines.py b/scipy/interpolate/tests/test_bsplines.py
index f555bb41778d..4ee648fe7b8e 100644
--- a/scipy/interpolate/tests/test_bsplines.py
+++ b/scipy/interpolate/tests/test_bsplines.py
@@ -592,6 +592,19 @@ def test_from_power_basis_exmp(self):
                                         bc_type='natural')
         assert_allclose(bspl.c, [1, 1, 1, 1, 1, 1, 1], atol=1e-15)
 
+    def test_read_only(self):
+        # BSpline must work on read-only knots and coefficients.
+        t = np.array([0, 1])
+        c = np.array([3.0])
+        t.setflags(write=False)
+        c.setflags(write=False)
+
+        xx = np.linspace(0, 1, 10)
+        xx.setflags(write=False)
+
+        b = BSpline(t=t, c=c, k=0)
+        assert_allclose(b(xx), 3)
+
 
 def test_knots_multiplicity():
     # Take a spline w/ random coefficients, throw in knots of varying
"
97,"DOC: Fixed error for levy and levy_l discriptions

Both discriptions incorrectly stated that the PDFs were valid at
x == 0, which would imply that it's possible to divide by 0.","diff --git a/scipy/stats/_continuous_distns.py b/scipy/stats/_continuous_distns.py
index 0bef50a409fc..6d653fd17ce1 100644
--- a/scipy/stats/_continuous_distns.py
+++ b/scipy/stats/_continuous_distns.py
@@ -5428,7 +5428,7 @@ class levy_gen(rv_continuous):
 
         f(x) = \frac{1}{\sqrt{2\pi x^3}} \exp\left(-\frac{1}{2x}\right)
 
-    for :math:`x >= 0`.
+    for :math:`x > 0`.
 
     This is the same as the Levy-stable distribution with :math:`a=1/2` and
     :math:`b=1`.
@@ -5531,7 +5531,7 @@ class levy_l_gen(rv_continuous):
     .. math::
         f(x) = \frac{1}{|x| \sqrt{2\pi |x|}} \exp{ \left(-\frac{1}{2|x|} \right)}
 
-    for :math:`x <= 0`.
+    for :math:`x < 0`.
 
     This is the same as the Levy-stable distribution with :math:`a=1/2` and
     :math:`b=-1`.
"
98,"DOC: adding references to the UnivariateSpline docstring

* DOC: adding references to the UnivariateSpline docstring, duplicate splrep references.

Reviewed at https://github.com/scipy/scipy/pull/18101","diff --git a/scipy/interpolate/_fitpack2.py b/scipy/interpolate/_fitpack2.py
index 6225400a35a6..f44249299cba 100644
--- a/scipy/interpolate/_fitpack2.py
+++ b/scipy/interpolate/_fitpack2.py
@@ -187,6 +187,21 @@ class UnivariateSpline:
     Notice the need to replace a ``nan`` by a numerical value (precise value
     does not matter as long as the corresponding weight is zero.)
 
+    References
+    ----------
+    Based on algorithms described in [1]_, [2]_, [3]_, and [4]_:
+
+    .. [1] P. Dierckx, ""An algorithm for smoothing, differentiation and
+       integration of experimental data using spline functions"",
+       J.Comp.Appl.Maths 1 (1975) 165-184.
+    .. [2] P. Dierckx, ""A fast algorithm for smoothing data on a rectangular
+       grid while using spline functions"", SIAM J.Numer.Anal. 19 (1982)
+       1286-1304.
+    .. [3] P. Dierckx, ""An improved algorithm for curve fitting with spline
+       functions"", report tw54, Dept. Computer Science,K.U. Leuven, 1981.
+    .. [4] P. Dierckx, ""Curve and surface fitting with splines"", Monographs on
+       Numerical Analysis, Oxford University Press, 1993.
+
     Examples
     --------
     >>> import numpy as np
"
